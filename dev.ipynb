{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from utils.caching import cache, ucache\n",
    "\n",
    "import os\n",
    "if not os.path.exists('out'):\n",
    "    os.makedirs('out')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reading and cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Book Summary Dataset](https://www.cs.cmu.edu/~dbamman/booksummaries.html) is used.\n",
    "\n",
    "The entries in the dataset (taken from its README file) are:\n",
    " 1. Wikipedia article ID\n",
    " 2. Freebase ID\n",
    " 3. Book title\n",
    " 4. Author\n",
    " 5. Publication date\n",
    " 6. Book genres (Freebase ID:name tuples)\n",
    " 7. Plot summary\n",
    "\n",
    "To save time, I rename them as follows:\n",
    "\n",
    "| Original name     | renaming     |\n",
    "| :---: | :---: |\n",
    "| Wikipedia article ID | id |\n",
    "| Freebase ID | f_id |\n",
    "| Book title | title |\n",
    "| Author | author |\n",
    "| Publication date | date |\n",
    "| Book genres (Freebase ID:name tuples) | genres |\n",
    "| Plot summary | summary |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"id\", \"f_id\", \"title\", \"author\", \"date\", \"genres\", \"summary\"]\n",
    "\n",
    "df_original = pd.read_csv('dataset/booksummaries.txt', sep='\\t', names=headers)\n",
    "print(f'Number of documents: {len(df_original)}')\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First make a dataframe for a nice visualization:\n",
    " - drop the Freebase ID column\n",
    " - convert the genres to lists of lowercase strings\n",
    "\n",
    "Then add the tokenized columns for `summary` and `title`:\n",
    " - convert everything to lowercase\n",
    " - keep only alphabetic characters (drop digits and punctuation)\n",
    " - strip accents\n",
    " - On the `summary` column:\n",
    "   - remove stopwords\n",
    "   - remove words that make no sense\n",
    "   - lemmatize words\n",
    "\n",
    "Build bag-of-words of each document, where the format is:\n",
    "\n",
    " - {word: n_occurrences}\n",
    "\n",
    "\n",
    "Finally, build the vocabulary of the summary. Only the words that appear in more than 1% of the documents are kept in the vocabulary, i.e the ones that appear in more than 17 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop freebase ID\n",
    "df = df_original.drop('f_id', axis=1)\n",
    "\n",
    "# convert genres to lists of strings\n",
    "df['genres'] = df['genres'].map(lambda x: list(json.loads(x.lower()).values()), na_action='ignore')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intitiate the tokenizer and lemmatizer\n",
    "tokenizer = RegexpTokenizer(r'[a-z]+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# convert stopwords to set for better performance\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "# define all mapping functions for proper tokenization\n",
    "def strip_accents(text: str):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', text)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def remove_single_chars(words: list):\n",
    "    return [word for word in words if len(word) > 1]\n",
    "\n",
    "def lemmatize(words: list):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "def remove_stopwords(words: list):\n",
    "    return [word for word in words if word not in stopwords_set]\n",
    "\n",
    "def apply_preprocessing(text: str):\n",
    "    return lemmatize(remove_stopwords(remove_single_chars(tokenize(strip_accents(text.lower())))))\n",
    "\n",
    "tqdm.pandas() # defines the progress_map function\n",
    "\n",
    "for key in ['title', 'summary']:\n",
    "    df[key + '_t'] = cache(f'prep_{key}.pck', df[key].progress_map, apply_preprocessing)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to build bag-of-words\n",
    "def build_bow(doc: list):\n",
    "    \"\"\"Count the words in each and build its bag-of-words\"\"\"\n",
    "    bow = {}\n",
    "    for word in doc:\n",
    "        if word not in bow.keys():\n",
    "            bow[word] = 0\n",
    "        bow[word]+=1\n",
    "    return dict(sorted(bow.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "\n",
    "# do it only on the summary\n",
    "t0 = time.time()\n",
    "# df['summary_set'] = df['summary_t'].map(lambda s: sorted(set(s))) # build the set just for better performance\n",
    "df['summary_bow'] = df['summary_t'].map(build_bow) # this enables to build the tf part of the tf-idf matrix\n",
    "t1 = time.time()\n",
    "print(f'execution time: {t1-t0:.2f}s')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the complete vocabulary and compute the document frequency\n",
    "vocabulary_complete = [word for doc in df['summary_bow'] for word in doc.keys()]\n",
    "\n",
    "from utils.document_frequency import compute_document_frequency\n",
    "document_frequency = cache('doc_freq.pck', compute_document_frequency, df, vocabulary_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the actual vocabulary\n",
    "min_df = 0.01\n",
    "print(f'Min number of document frequency: {len(df)*min_df}')\n",
    "vocabulary = [token for token, freq in document_frequency.items() if freq > len(df)*min_df]\n",
    "\n",
    "# create mappings for vocabulary\n",
    "token2id = {word: i for i, word in enumerate(vocabulary)}\n",
    "id2token = {i: word for word, i in token2id.items()}\n",
    "\n",
    "print(f'Vocabulary size: {len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the tf-idf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf(df, id2token):\n",
    "    tf_idf = np.zeros((len(vocabulary), len(df)))\n",
    "    for i, word in tqdm(id2token.items()):\n",
    "        for j, bow in enumerate(df['summary_bow']):\n",
    "            if word in bow:\n",
    "                tf_idf[i,j] = np.log(1+bow[word])*np.log(document_frequency[word])\n",
    "    return tf_idf\n",
    "\n",
    "\n",
    "tf_idf = cache('if-idf.pck', compute_tf_idf, df, id2token)\n",
    "print(f'{tf_idf.shape[0]} words and {tf_idf.shape[1]} documents.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA on tf-idf matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform SVD on the tf-idf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vt = cache('svd_tf_idf.pck', np.linalg.svd, tf_idf, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\"Hercule Poirot's Christmas\", \"Murder on the Orient express\",\n",
    "          \"Nightfall\", \"Robots and Empire\", \"Foundation\", \"Second Foundation\",\n",
    "          \"Harry potter and the Philosopher's stone\",\n",
    "          \"The fellowship of the Ring\", \"The Two Towers\", \"The Return of the King\", \"The Hobbit\",\n",
    "          \"The da Vinci Code\", \"Angels and Demons\",\n",
    "          \"Pride and Prejudice\",\n",
    "          \"The Shining\",\n",
    "          \"Moby-Dick; or, The Whale\", \"A Farewell to Arms\"]\n",
    "\n",
    "doc_subset = list(df[(df['title'].map(lambda t: t in titles) & ~df['author'].isna() & ~df['genres'].isna())].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[doc_subset][['author', 'title', 'genres']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA space of documents\n",
    "\n",
    "Visualize the first 2 LSA dimensions for the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualization import plot_docs, animate_k\n",
    "from utils.caching import ext_cache, get_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, it is a good idea to tune the value of k. This is achieved by creating and animation using the FuncAnimation class from matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [2, 3, 4, 5, 10] + list(range(20, 110, 10)) + list(range(100, 1100, 100)) + [2000, 5000, 10000, vt.shape[1]]\n",
    "animate_k('out/docs_animation.mp4', k_values, plot_docs,\n",
    "          vt=vt, s=s, dimensions=(0,1), normalize=True, scatter_kw={'s': 1, 'alpha': 0.3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [3, 4, 5, 10] + list(range(20, 110, 10)) + list(range(100, 1100, 100)) + [2000, 5000, 10000, vt.shape[1]]\n",
    "animate_k('out/docs_animation_2.mp4', k_values, plot_docs,\n",
    "          vt=vt, s=s, dimensions=(1,2), normalize=True, scatter_kw={'s': 1, 'alpha': 0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the animations, the best spread of the data points occurs at around $k=100$.\n",
    "\n",
    "Therefore, from now on, for the docs this value for k will be chosen.\n",
    "\n",
    "Here is a visualization of the unnormalized LSA vectors (left), and the normalized k-rank approximated ones for LSA dimensions (0,1) (right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h, dpi = 640*2, 640, 100\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "# a random subsample is shown, because with all the points my PC does not handle the rendering very well\n",
    "plot_docs(vt, s, (0,1), k=100, normalize=False, ax=axs[0], scatter_kw={'s': 1, 'alpha': 0.3}, subsample_size=1000)\n",
    "plot_docs(vt, s, (0,1), k=100, normalize=True,  ax=axs[1], scatter_kw={'s': 1, 'alpha': 0.3}, subsample_size=1000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for LSA dimensions (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h, dpi = 640*2, 640, 100\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "plot_docs(vt, s, (1,2), k=100, normalize=False, ax=axs[0], scatter_kw={'s': 1, 'alpha': 0.3}, subsample_size=1000)\n",
    "plot_docs(vt, s, (1,2), k=100, normalize=True,  ax=axs[1], scatter_kw={'s': 1, 'alpha': 0.3}, subsample_size=1000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display some specific books\n",
    "\n",
    "Show only some specific books. The commented cells were used to check if the Title I want to show was present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = \"A Farewell to Arms\"\n",
    "# df.loc[df['title'].map(lambda s: s.lower()).str.contains(t.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df['title'].str.lower() == t.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df['author'].str.lower() == 'Ernest Hemingway'.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\"Hercule Poirot's Christmas\", \"Murder on the Orient express\",\n",
    "          \"Nightfall\", \"Robots and Empire\", \"Foundation\", \"Second Foundation\",\n",
    "          \"Harry potter and the Philosopher's stone\",\n",
    "          \"The fellowship of the Ring\", \"The Two Towers\", \"The Return of the King\", \"The Hobbit\",\n",
    "          \"The da Vinci Code\", \"Angels and Demons\",\n",
    "          \"Pride and Prejudice\",\n",
    "          \"The Shining\",\n",
    "          \"Moby-Dick; or, The Whale\", \"A Farewell to Arms\"]\n",
    "\n",
    "doc_subset = list(df[(df['title'].map(lambda t: t in titles) & ~df['author'].isna() & ~df['genres'].isna())].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h, dpi = 640*2, 640, 70\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "plot_docs(vt, s, (0,1), k=100, normalize=False, ax=axs[0], subset=doc_subset, labels=df['title'].to_numpy())\n",
    "plot_docs(vt, s, (0,1), k=100, normalize=True,  ax=axs[1], subset=doc_subset, labels=df['title'].to_numpy())\n",
    "\n",
    "axs[0].set_xlim(-80,0)\n",
    "axs[0].set_ylim(-50, 30)\n",
    "\n",
    "axs[1].set_xlim(-1.02,0)\n",
    "axs[1].set_ylim(-0.51,0.51)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA space of words\n",
    "\n",
    "Analogue to before, but now let's see the similarities between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualization import plot_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning of the approximation level k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [2, 3, 4, 5, 10] + list(range(20, 110, 10)) + list(range(100, 1100, 100)) + [1500, 2000, u.shape[1]]\n",
    "animate_k('out/words_animation.mp4', k_values, plot_words,\n",
    "          u=u, s=s, dimensions=(0,1), normalize=True, scatter_kw={'s': 1, 'alpha': 0.3})\n",
    "\n",
    "animate_k('out/words_animation_2.mp4', k_values[1:], plot_words,\n",
    "          u=u, s=s, dimensions=(1,2), normalize=True, scatter_kw={'s': 1, 'alpha': 0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, some specific words are chosen. I have chosen 4 different categories of words. As it turns out, using small values of k results in the best visualization for the clustering of these specific words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_of_interest = ['love', 'marriage', 'parent', 'school',\n",
    "                     'dark', 'fight', 'criminal',\n",
    "                     'alien', 'spaceship', 'planet',\n",
    "                     'car', 'truck', 'bus', 'train']\n",
    "\n",
    "words_subset = []\n",
    "voc = np.array(vocabulary)\n",
    "for word_of_interest in words_of_interest:\n",
    "    words_subset += list(np.indices(dimensions=voc.shape)[0][voc==word_of_interest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [2, 3, 4, 5, 10] + list(range(20, 110, 10)) + list(range(100, 1100, 100)) + [1500, 2000, u.shape[1]]\n",
    "animate_k('out/words_animation_woi.mp4', k_values, plot_words,\n",
    "          u=u, s=s, labels=vocabulary, subset=words_subset)\n",
    "\n",
    "animate_k('out/words_animation_woi_2.mp4', k_values[1:], plot_words,\n",
    "          u=u, s=s, dimensions=(1,2), labels=vocabulary, subset=words_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with $k=3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "\n",
    "w, h, dpi = 640*2, 640, 100\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "plot_words(u, s, labels=vocabulary, ax=axs[0], subset=words_subset, k=k, dimensions=(0,1))\n",
    "plot_words(u, s, labels=vocabulary, ax=axs[1], subset=words_subset, k=k, dimensions=(1,2))\n",
    "axs[0].set_xlim(-1.02,0)\n",
    "axs[0].set_ylim(-0.51,0.51)\n",
    "\n",
    "axs[1].set_xlim(-1.02,1.02)\n",
    "axs[1].set_ylim(-1.02,1.02)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'drama' in df['genres'].explode().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['science fiction', 'fantasy', 'drama']\n",
    "df['genres'].map(lambda s: len(set(genres).intersection(s))>=2, na_action='ignore').map(lambda s: False if pd.isna(s) else s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualization import plot_genres_analysis\n",
    "plot_genres_analysis(vt, s, df, genres=['science fiction', 'fantasy'], normalize=True, k=100, u=u, voc=vocabulary, words=['spaceship'],\n",
    "                     plot_most_relevant_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curvature of the singular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_function(x):\n",
    "    return x[1:] - x[:-1]\n",
    "\n",
    "def curvature_function(s):\n",
    "    s_diff = diff_function(diff_function(s))\n",
    "    return s_diff/((1+np.square(s_diff))**1.5)\n",
    "\n",
    "w, h, dpi = 640, 360, 100\n",
    "fig, ax = plt.subplots(figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "ax.plot(curvature_function(s)[:30], 'o-')\n",
    "ax.grid()\n",
    "fig.suptitle('Curvature function')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 13\n",
    "u_k = u[:, :k]\n",
    "s_k = s[:k]\n",
    "vt_k = vt[:k]\n",
    "\n",
    "tf_idf_k = u_k@np.diag(s_k)@vt_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.similarities import compute_most_similar_movies_lsa\n",
    "from utils.similarities import compute_cos_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_k*vt_k[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_vectors = np.reshape(s_k, newshape=(s_k.shape[0],1))*vt_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cos_similarities(s_k*vt_k[:,0], movies_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_row = df[df['title'].map(lambda t: 'the plague'.lower()==t.lower())]\n",
    "movie_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low-rank approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_term_similarities(terms_docs):\n",
    "    terms_docs_norm = terms_docs.T/np.linalg.norm(terms_docs, axis=1)\n",
    "    terms_docs_norm = np.where(np.isnan(terms_docs_norm), 0, terms_docs_norm)\n",
    "    terms_docs_norm = terms_docs_norm.T\n",
    "\n",
    "    similarities = terms_docs_norm@terms_docs_norm.T\n",
    "    for i in range(len(similarities)):\n",
    "        similarities[i,i]=0\n",
    "    return similarities\n",
    "\n",
    "def compute_doc_similarities(terms_docs):\n",
    "    terms_docs_norm = terms_docs/np.linalg.norm(terms_docs, axis=0)\n",
    "    terms_docs_norm = np.where(np.isnan(terms_docs_norm), 0, terms_docs_norm)\n",
    "\n",
    "    similarities = terms_docs_norm.T@terms_docs_norm\n",
    "    for i in range(len(similarities)):\n",
    "        similarities[i,i]=0\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_similarities = cache('term_similarities.pck', compute_term_similarities, tf_idf_k)\n",
    "doc_similarities = cache('doc_similarities.pck', compute_doc_similarities, tf_idf_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_similarities[].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = 0\n",
    "d2 = 813\n",
    "print(df['title'][d1])\n",
    "print(df['title'][d2])\n",
    "doc_similarities[d1,d2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'][9296]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['title'].str.lower().str.contains('fahrenheit')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['title'].str.lower().str.contains('foundation')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary'][9296]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
