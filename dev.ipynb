{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from utils.caching import cache, ucache\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reading and cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Book Summary Dataset](https://www.cs.cmu.edu/~dbamman/booksummaries.html) is used.\n",
    "\n",
    "The entries in the dataset (taken from its README file) are:\n",
    " 1. Wikipedia article ID\n",
    " 2. Freebase ID\n",
    " 3. Book title\n",
    " 4. Author\n",
    " 5. Publication date\n",
    " 6. Book genres (Freebase ID:name tuples)\n",
    " 7. Plot summary\n",
    "\n",
    "To save time, I rename them as follows:\n",
    "\n",
    "| Original name     | renaming     |\n",
    "| :---: | :---: |\n",
    "| Wikipedia article ID | id |\n",
    "| Freebase ID | f_id |\n",
    "| Book title | title |\n",
    "| Author | author |\n",
    "| Publication date | date |\n",
    "| Book genres (Freebase ID:name tuples) | genres |\n",
    "| Plot summary | summary |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"id\", \"f_id\", \"title\", \"author\", \"date\", \"genres\", \"summary\"]\n",
    "\n",
    "df_original = pd.read_csv('dataset/booksummaries.txt', sep='\\t', names=headers)\n",
    "print(f'Number of documents: {len(df_original)}')\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First make a dataframe for a nice visualization:\n",
    " - drop the Freebase ID column\n",
    " - convert the genres to lists of lowercase strings\n",
    "\n",
    "Then add the tokenized columns for `summary` and `title`:\n",
    " - convert everything to lowercase\n",
    " - keep only alphabetic characters (drop digits and punctuation)\n",
    " - strip accents\n",
    " - On the `summary` column:\n",
    "   - remove stopwords\n",
    "   - remove words that make no sense\n",
    "   - lemmatize words\n",
    "\n",
    "Build bag-of-words of each document, where the format is:\n",
    "\n",
    " - {word: n_occurrences}\n",
    "\n",
    "\n",
    "Finally, build the vocabulary of the summary. Only the words that appear in more than 1% of the documents are kept in the vocabulary, i.e the ones that appear in more than 17 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop freebase ID\n",
    "df = df_original.drop('f_id', axis=1)\n",
    "\n",
    "# convert genres to lists of strings\n",
    "df['genres'] = df['genres'].map(lambda x: list(json.loads(x.lower()).values()), na_action='ignore')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intitiate the tokenizer and lemmatizer\n",
    "tokenizer = RegexpTokenizer(r'[a-z]+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# convert stopwords to set for better performance\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "# define all mapping functions for proper tokenization\n",
    "def strip_accents(text: str):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', text)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def remove_single_chars(words: list):\n",
    "    return [word for word in words if len(word) > 1]\n",
    "\n",
    "def lemmatize(words: list):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "def remove_stopwords(words: list):\n",
    "    return [word for word in words if word not in stopwords_set]\n",
    "\n",
    "def apply_preprocessing(text: str):\n",
    "    return lemmatize(remove_stopwords(remove_single_chars(tokenize(strip_accents(text.lower())))))\n",
    "\n",
    "tqdm.pandas() # defines the progress_map function\n",
    "\n",
    "for key in ['title', 'summary']:\n",
    "    df[key + '_t'] = cache(f'prep_{key}.pck', df[key].progress_map, apply_preprocessing)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to build bag-of-words\n",
    "def build_bow(doc: list):\n",
    "    \"\"\"Count the words in each and build its bag-of-words\"\"\"\n",
    "    bow = {}\n",
    "    for word in doc:\n",
    "        if word not in bow.keys():\n",
    "            bow[word] = 0\n",
    "        bow[word]+=1\n",
    "    return dict(sorted(bow.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "\n",
    "# do it only on the summary\n",
    "t0 = time.time()\n",
    "# df['summary_set'] = df['summary_t'].map(lambda s: sorted(set(s))) # build the set just for better performance\n",
    "df['summary_bow'] = df['summary_t'].map(build_bow) # this enables to build the tf part of the tf-idf matrix\n",
    "t1 = time.time()\n",
    "print(f'execution time: {t1-t0:.2f}s')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the complete vocabulary and compute the document frequency\n",
    "vocabulary_complete = [word for doc in df['summary_bow'] for word in doc.keys()]\n",
    "\n",
    "from utils.document_frequency import compute_document_frequency\n",
    "document_frequency = cache('doc_freq.pck', compute_document_frequency, df, vocabulary_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the actual vocabulary\n",
    "min_df = 0.01\n",
    "print(f'Min number of document frequency: {len(df)*min_df}')\n",
    "vocabulary = [token for token, freq in document_frequency.items() if freq > len(df)*min_df]\n",
    "\n",
    "# create mappings for vocabulary\n",
    "token2id = {word: i for i, word in enumerate(vocabulary)}\n",
    "id2token = {i: word for word, i in token2id.items()}\n",
    "\n",
    "print(f'Vocabulary size: {len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First build the term-docs matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_terms_docs(df, id2token):\n",
    "    term_docs = np.zeros((len(vocabulary), len(df)))\n",
    "    for i, word in tqdm(id2token.items()):\n",
    "        for j, bow in enumerate(df['summary_bow']):\n",
    "            if word in bow:\n",
    "                term_docs[i,j] = bow[word]\n",
    "    return term_docs\n",
    "\n",
    "terms_docs = cache('terms_docs.pck', compute_terms_docs, df, id2token)\n",
    "print(f'{terms_docs.shape[0]} words and {terms_docs.shape[1]} documents.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf(df, id2token):\n",
    "    tf_idf = np.zeros((len(vocabulary), len(df)))\n",
    "    for i, word in tqdm(id2token.items()):\n",
    "        for j, bow in enumerate(df['summary_bow']):\n",
    "            if word in bow:\n",
    "                tf_idf[i,j] = np.log(1+bow[word])*np.log(document_frequency[word])\n",
    "    return tf_idf\n",
    "\n",
    "\n",
    "tf_idf = cache('if-idf.pck', compute_tf_idf, df, id2token)\n",
    "print(f'{tf_idf.shape[0]} words and {tf_idf.shape[1]} documents.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA on tf-idf matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then compute the tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vt = cache('svd_tf_idf.pck', np.linalg.svd, tf_idf, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h, dpi = 960, 360, 100\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "axs[0].plot(s)\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].plot(s[:30], '.-')\n",
    "axs[1].grid()\n",
    "\n",
    "fig.suptitle('Singular values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_function(x):\n",
    "    return x[1:] - x[:-1]\n",
    "\n",
    "def curvature_function(s):\n",
    "    s_diff = diff_function(diff_function(s))\n",
    "    return s_diff/((1+np.square(s_diff))**1.5)\n",
    "\n",
    "w, h, dpi = 640, 360, 100\n",
    "fig, ax = plt.subplots(figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "ax.plot(curvature_function(s)[:30], 'o-')\n",
    "ax.grid()\n",
    "fig.suptitle('Singular values')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low-rank approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 13\n",
    "u_k = u[:, :k]\n",
    "s_k = s[:k]\n",
    "vt_k = vt[:k]\n",
    "\n",
    "tf_idf_k = u_k@np.diag(s_k)@vt_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(v1, v2):\n",
    "    return (v1/np.linalg.norm(v1)).dot(v2/np.linalg.norm(v2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
